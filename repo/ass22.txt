#%%
df = spark.read.csv("/Volumes/workspace/internship/skit/sales.csv", header=True, inferSchema=True)
display(df)

#%%
from pyspark.sql.functions import expr

df_filter = df.groupBy( "ORDERNUMBER", "QUANTITYORDERED", "PRICEEACH", "ORDERLINENUMBER", "SALES").agg(
    expr(
        "sum(ORDERNUMBER + QUANTITYORDERED + PRICEEACH + ORDERLINENUMBER + SALES) as agg_sales_data"
    )
)
display(df_filter)
#%%
df_select = df_filter.select("ORDERNUMBER", "QUANTITYORDERED", "PRICEEACH", "ORDERLINENUMBER", "SALES", "agg_sales_data").where("SALES > 10000")
display(df_select)
#%%
df_select.write.format("delta").mode("overwrite").saveAsTable("workspace.internship.delta_able")
#%%
df_history = spark.sql("DESCRIBE HISTORY workspace.internship.delta_able")
display(df_history)
#%%
df_delta = spark.read.format("delta").table("workspace.internship.delta_able")
display(df_delta)
#%%
df_partition = df_filter.write.partitionBy("SALES").format("delta").mode("overwrite").saveAsTable("workspace.internship.delta_partition")